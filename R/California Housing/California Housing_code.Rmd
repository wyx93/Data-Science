---
title: "California Housing Project"
author: "Sara (Yi-Ning) Huang, Stella (Yuxin) Wan"
date: "11/15/2019"
output:
  html_document:
    toc: true
    toc_float: TRUE
---

# Project & Data Introduction

This dataset was basically built using the 1990 California census data, and was appeared in a 1997 paper titled Sparse Spatial Autoregressions by Pace, R. Kelley and Ronald Barry, published in the Statistics and Probability Letters journal. This version is a modified version available from a professor Lu√≠s Torgo's page, and he obtained it from the StatLib repository (which is closed now). Each row of data contain a census block group, which is the smallest geographical unit for U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3000 people).

The purpose of this project is to explore this "California Housing" data and evaluate three machine learning algorithms to address 2 questions that we are interested in:

**1. What are the relevant variables and how they influence the house price? How to predict house price?**
**2. For people having different levels of income, do they have any preferences for housing properties?**

We will discuss importance and selection of variables, determine optimal tuning parameter values, display the outputs of models, compare models' performances, and make conclusions for these two questions.

# Three Machine Learning Methods

1. Linear Regression
2. Polynomial Regression
3. Ridge Regression

# Question 1: How to predict house price?
```{r echo = TRUE, results = "hide", message = FALSE}
## Prepare packages
library(ggplot2)
library(gridExtra)
library(caret)
library(car)
library(ridge)
library(glmnet)
library(dplyr)
```

```{r}
## Preapare data
## Read dataset
hd <- read.csv("housing.csv")

## Clean dataset
## Remove the rows of missing values
hd <- hd[which(!is.na(hd$total_bedrooms)), ]

## Split data into train & test data
set.seed(123)
training.samples <- hd$median_house_value %>%
  createDataPartition(p = 0.8, list = FALSE)
train_data  <- hd[training.samples, ]
test_data <- hd[-training.samples, ]

```

## 1. Linear Regression
### 1.1 Fit Multiple Linear Regression Model with All variables
```{r}
## Use multiple linear regression
lrm <- lm(log(median_house_value) ~ longitude + latitude + housing_median_age + total_rooms + total_bedrooms + population + households + ocean_proximity + median_income, data = train_data)
summary(lrm)

## Do cross validation
## Make predictions
lrm_pred <- predict(lrm, test_data)

## Compute the model performance using RMSE & R2
mp_lrm <- data.frame( RMSE = RMSE(lrm_pred, test_data$median_house_value),
                      R2 = R2(lrm_pred, test_data$median_house_value))
mp_lrm

```

> Findings & Explanation: 

It seems that all variables have non-ignorable impact on house price (p < 0.05). However, we need to take a closer look to check if these variables really have linearity.

### 1.2 Check Linearity for relationship between each variable & house price
* Geographic variables
```{r}
## Plot the relationship between each variable & median_house_value

## median_house_value ~ longitutde
plot1 <- ggplot(hd, aes(x = longitude, y = median_house_value)) +
  geom_point(data = hd, aes(x = longitude, y = median_house_value)) +
  labs(title = "House Price Factor", y = "House Price", x = "Longitude")
## median_house_value ~ latitude
plot2 <- ggplot(hd, aes(x = latitude, y = median_house_value)) +
  geom_point(data = hd, aes(x = latitude, y = median_house_value)) +
  labs(title = "House Price Factor", y = "House Price", x = "Latitude")
grid.arrange(plot1, plot2, nrow = 1)

## Calculate the distance of the two peaks
## A degree of longitude = 69.172 miles/ A degree of latitude = 55 miles
## Demonstrate two peaks are SF & LA (straight line distance = 350 miles)
sqrt(((122.4 - 118.2) * 70)^2 + ((37.8 - 34.1) * 55)^2)

## Plot the geography, demonstrate this census data cover the whole California
ggplot(hd, aes(x = longitude, y = latitude)) +
  geom_point(data = hd, aes(x = longitude, y = latitude)) +
  labs(title = "Geographic Distribution", y = "Latitude", x = "Longitude")
## Compared to the map of California
knitr::include_graphics("california-lat-long-map.jpg")
```

```{r echo = TRUE, results = "hide", message = FALSE}
library(plotly)
```

```{r}
## Plot the 3D plot to see the house price distribution based on california geography
p <- plot_ly(hd, x = ~latitude, y = ~(-longitude), z = ~median_house_value, marker = list(color = ~median_house_value, colorscale = c('#FFE1A1', '#683531'), showscale = TRUE)) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'Latitude'),
                     yaxis = list(title = 'Longitude'),
                     zaxis = list(title = 'House Price'))) 
p

## median_house_value ~ ocean_proximity
ggplot(hd, aes(x = longitude, y = latitude)) +
  geom_point(data = hd, aes(color = factor(ocean_proximity))) +
  labs(title = "Geographic Distribution by Ocean Proximity", y = "Latitude", x = "Longitude")
## Plot the boxplot to see the house distribution based on ocean proximity
boxplot(median_house_value ~ ocean_proximity, data = hd)

## See the houses number in these 5 areas
table(hd$ocean_proximity)

```

> Findings & Explanation: 

1. Based on the 2D & 3D plots, we know that this census data cover the whole California State. The locations of the two peaks shown in the longitude & latitude plots are San Francisco & Los Angeles.
2. Based on the 3D plot, the house price is highly relevant to locations (e.g. urban area and rural area). However, the fitting should be based on different area categorization, not longitude & lattitude. Since latitude and longitude are geographic factors, it is not reasonable that they have simple linearity relationships with house prices. Therefore, we do not consider these two factors in linear regression model.
3. Instead of using longitude & latitude as variables, we will choose obervations with certain conditions that may reveal the effect of location on house price in part 1.3.
4. Based on the boxplot of ocean proximity & median house price, without considering island houses (<0.1%), the average median income of people living near bay or ocean are higher than that of people living inland. Besides, ocean proximity is categorical variable, they are not considered in linear regression as well.

```{r echo = FALSE}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```

* Numeric variables
```{r}
library(ggplot2)

## Check linearity by scatter plots & pearson correlation coefficients

## Use a list to store each plot
myplots <- vector('list', ncol(hd[3:8]))
## Create a function to plot scatter plots
plotfun <- function(data, col) {
  ggplot(data, aes(x = data[, col], y = median_house_value)) +
  geom_point(data = data, aes(x =  data[, col], y = median_house_value)) +
  labs(title = "House Price Factor", y = "House Price", x = colnames(data)[col])
}

## Apply the function to the 6 numeric variables
myplots <- lapply(c(3:8), plotfun, data = hd)
## Display 6 scatter plots
multiplot(plotlist = myplots, cols = 2)

## Create a for loop to calculate the correlation coefficient
cc <- vector()
for (i in 3:8) {
  print(paste("Calculating the correlation coefficient of", colnames(hd)[i], "& house price", sep = " "))
  cc[i - 2] <- cor(hd[, i], hd$median_house_value, method = "pearson")
}
cc

```

> Findings & Explanation:

1. Based on the scatter plots and pearson correlation coefficients, the variables including house median age, total rooms, total bedrooms, households & population and the responsive variable house price are considered not irrelevant. Hence we can say that no obvious linearity is observed.
2. However, there is a possible correlation between median income & house prize. Of all the variables, best predictor for the house price is median income.
3. House price might be related to these variables, but other relevant variables (such as location) outweight their importance, and they need to be considered in first priority.

### 1.3 Extract group data to do explorative analysis
```{r}

## Group 1: Study the "1st quantile to 2nd quantile house price" houses
hist(hd$median_house_value)
summary(hd$median_house_value)
firstq <- unname(quantile(hd$median_house_value, 0.25))
secondq <- unname(quantile(hd$median_house_value, 0.50))

hd2 <- hd %>%
  filter(median_house_value >= firstq & median_house_value <= secondq)

## Check linearity by scatter plots & pearson correlation coefficients
## Use a list to store each plot
myplots2 <- vector('list', ncol(hd2[3:8]))
## Apply the plot function to the 6 numeric variables
myplots2 <- lapply(c(3:8), plotfun, data = hd2)
## Display 6 scatter plots
multiplot(plotlist = myplots2, cols = 2)

## Create a for loop to calculate the correlation coefficients
cc2 <- vector()
for (i in 3:8) {
  print(paste("Calculating the correlation coefficient of", colnames(hd2)[i], "& house price", sep = " "))
  cc2[i - 2] <- cor(hd2[, i], hd2$median_house_value, method = "pearson")
}
cc2

## Group 2: Study the "Los Angeles" houses
hd3 <- hd %>%
  filter(longitude <= -117.6 & longitude >= -118.7 & latitude >= 33.8 & latitude <= 34.9)

## Check linearity by scatter plots & pearson correlation coefficients
## Use a list to store each plot
myplots3 <- vector('list', ncol(hd2[3:8]))
## Apply the plot function to the 6 numeric variables
myplots3 <- lapply(c(3:8), plotfun, data = hd3)
## Display 6 scatter plots
multiplot(plotlist = myplots3, cols = 2)

## Create a for loop to calculate the correlation coefficients
cc3 <- vector()
for (i in 3:8) {
  print(paste("Calculating the correlation coefficient of", colnames(hd3)[i], "& house price", sep = " "))
  cc3[i - 2] <- cor(hd3[, i], hd3$median_house_value, method = "pearson")
}
cc3

## Group3 : Study the "LA houses with other factors fixed" houses
hd4 <- hd %>%
  filter(longitude <= -117.6 & longitude >= -118.7 & latitude >= 33.8 & latitude <= 34.9) %>%
  filter(housing_median_age <= quantile(housing_median_age, 0.25)) %>%
  filter(total_rooms <= quantile(total_rooms, 0.25)) %>%
  filter(total_bedrooms <= quantile(total_bedrooms, 0.25))

## Check linearity by scatter plots & pearson correlation coefficients
## Use a list to store each plot
myplots4 <- vector('list', ncol(hd2[3:8]))
## Apply the plot function to the 6 numeric variables
myplots4 <- lapply(c(3:8), plotfun, data = hd4)
## Display 6 scatter plots
multiplot(plotlist = myplots4, cols = 2)

## Create a for loop to calculate the correlation coefficients
cc4 <- vector()
for (i in 3:8) {
  print(paste("Calculating the correlation coefficient of", colnames(hd4)[i], "& house price", sep = " "))
  cc4[i - 2] <- cor(hd4[, i], hd4$median_house_value, method = "pearson")
}
cc4

## Summary all correlation coefficients
cc_all <- round(data.frame(cc, cc2, cc3, cc4), 3)
cc_all

## Plot median income vs. house price by ocean proximity
ggplot(hd, aes(x = median_income, y = median_house_value)) +
 geom_point(data = hd, aes(color = factor(ocean_proximity))) +
 labs(title = "HP by Ocean Proximity", y = "House Price", x = "Income")

```

> Findings & Explanation:

We extract three different groups (fixed range of house price, LA, LA with other factors fixed) to study if there is linearity relationship between x & y. Results showed that except median income, other factors are not relevant to house price. In each group, no correlation coefficients have increased, thus these factors are not applied to lrm.

### 1.4 Fit simple linear regression with relevant variable: median income
```{r}
## Use linear regression with the best predictor
lrm_fix <- lm(median_house_value ~ median_income, data = train_data)
summary(lrm_fix)

## Do cross validation
## Make predictions
lrm_fix_pred <- predict(lrm_fix, test_data)

## Compute the model performance
mp_lrm_fix <- data.frame( RMSE = RMSE(lrm_fix_pred, test_data$median_house_value),
                      R2 = R2(lrm_fix_pred, test_data$median_house_value))
mp_lrm_fix

## Plot the simple lrm model fit
ggplot(test_data, aes(x = median_income, y = median_house_value)) +
  geom_point(data = test_data, aes(x = median_income, y = median_house_value)) +
  labs(title = "House Price Factor", y = "House Price", x = "median_income") +
  geom_line(aes(y = lrm_fix_pred, color = "Simple linear regression"))

```

> Findings & Explanation:

Overall, simple linear regression can explain the main trend of house price by median income. However, there is still great variance for the estimates.

## 2. Polynomial Regression
```{r}
## Use polynomial regression
poly_mod2 <- lm(median_house_value ~ poly(median_income, 2), data = train_data)
poly_mod3 <- lm(median_house_value ~ poly(median_income, 3), data = train_data)
poly_mod4 <- lm(median_house_value ~ poly(median_income, 4), data = train_data)
sumlist <- list(summary(poly_mod2), summary(poly_mod3), summary(poly_mod4))
sumlist

## Do cross validation
## Make predictions
poly_mod2_pred <- predict(poly_mod2, test_data)
poly_mod3_pred <- predict(poly_mod3, test_data)
poly_mod4_pred <- predict(poly_mod4, test_data)

## Compute the model performance
mp_poly_mod2 <- data.frame( RMSE = RMSE(poly_mod2_pred, test_data$median_house_value),
                      R2 = R2(poly_mod2_pred, test_data$median_house_value))
mp_poly_mod3 <- data.frame( RMSE = RMSE(poly_mod3_pred, test_data$median_house_value),
                      R2 = R2(poly_mod3_pred, test_data$median_house_value))
mp_poly_mod4 <- data.frame( RMSE = RMSE(poly_mod4_pred, test_data$median_house_value),
                      R2 = R2(poly_mod4_pred, test_data$median_house_value))
mp_poly_all <- rbind(poly_mod2 = mp_poly_mod2, poly_mod3 = mp_poly_mod3, poly_mod4 = mp_poly_mod4)
mp_poly_all

## Plot polynomidal model fits in one graph
## median_house_value ~ median_income
ggplot(test_data, aes(x = median_income, y = median_house_value)) +
  geom_point(data = test_data, aes(x = median_income, y = median_house_value)) +
  labs(title = "House Price Factor", y = "House Price", x = "median_income") +
  geom_line(aes(y = poly_mod2_pred, color = "Polynomidal Model with n = 2")) +
  geom_line(aes(y = poly_mod3_pred, color = "Polynomidal Model with n = 3")) +
  geom_line(aes(y = poly_mod4_pred, color = "Polynomidal Model with n = 4"))


```

> Findings & Explanation: 

1. We use the most relevant predictor median_income to predict the house price by using polynomial regression. 2nd, 3rd and 4th degree of polynomial regression have been conducted. Cross validation is also done to see model performances.
2. Comparing the three models, although 4th degree of polynomial regression has the smallest RMSE & largest R-squared value, it might be the result of over-fitting, as we can see the coefficient of the 4th term is considered irrelevant. 2nd degree of polynomial regression can not predict the house price accurately when median income is too large. Hence, 3rd polynomial regression is chosen as a better fit.
3. Overall, polynomidal regression can explain the main trend of house price by median income. However, there is still great variance for the estimates.

## 3. Ridge Regression
```{r}
## Fit ridge regression

## Predictor variables
x <- as.matrix(train_data[, c(3:8)])
## Outcome variable
y <- train_data$median_house_value

## Select the best lambda using cross-validation
## cv.glmnet is a function that can automatically generate lamda w/ least standard error
set.seed(123) 
cv <- cv.glmnet(x, y, alpha = 0)
## Display the best lambda value
cv$lambda.min

# Fit the ridge model (alpha = 0) on the training data
ridge_mod <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
# Display regression coefficients
coef(ridge_mod)

# Make predictions on the test data
x_test <- as.matrix(test_data[, c(3:8)])
ridge_mod_pred <- predict(ridge_mod, x_test) %>% as.vector()
# Model performance metrics
mp_ridge <- data.frame(
  RMSE = RMSE(ridge_mod_pred, test_data$median_house_value),
  R2 = R2(ridge_mod_pred, test_data$median_house_value)
)
mp_ridge


```

> Findings & Explanation:

Ridge regression is applied to the dataset with 6 variables: housing age, total rooms, total bedrooms, population, housholds, median income. The cross validation test error is 75687 and R2 is 0.57. Selection of tuning parameter lambda is determined based on the least standard error of cross validation. The best lambda value is 7905.

```{r}
mp_all <- rbind(lrm = mp_lrm_fix, poly = mp_poly_mod3, ridge = mp_ridge)
mp_all
```

## 4. Conclusion for Question 1

##### Linear regression is a useful and widely used tool for predicting a quantitative response. In this question, we explored the associations between all variables and the response, and found that except median income, other are not suitable variables to predict house price. Therefore, simple linear regression is implemented by using median income. We also used polynomial regression to predict house price by median income. Although these two models are good to interpret the results, the estimators have great variance, indicating that they have concerns about prediction accuracy. This can be demonstrated by the cross validation results in the above table. Unsuprisingly, linear & polynomial regression do have higher standard error.

##### Based on the model performance comparison, ridge regresssion is demonstrated as the better model because it has the smallest RMSE and the largest R2. This happens because as lambda increases, the flexibility of ridge regression fit decreases, leading to decreased variance and increased bias. At the expense of slight increase in bias, in this case, ridge regression model help increase the prediction accuracy.

# Question 2: How to predict preferences for housing properties?

#### Set up
```{r echo = TRUE, results = "hide", message = FALSE}
library(tidyverse)
library(readxl)
library(tidyr)
library(data.table)
library(ggplot2)
library(XML)
library(rvest)
library(dplyr)
library(openxlsx)
library(readr)
library(caret)
library(ISLR)

hd2 <- read.csv("housing.csv")
housing <- hd2
```


## 1. Linear Regression
```{r}
m2<- lm(longitude ~ median_income, data = housing)
summary(m2)
plot(housing$median_income, housing$longitude)
abline(lm(housing$longitude ~ housing$median_income), col='red')
```

> Findings & Explanation: 

We know that median_income does not have a clear effect on longtitude. But rich people prefer SF and LA.(The latitude of about -118 is in LA and the latitude of about -122 is in San Francisco.) There is big difference between different areas so it does not make sense to build linear regression model on the whole dataset.

```{r}
m3<- lm(latitude ~ median_income, data = housing)
summary(m3)
plot(housing$median_income, housing$latitude)
abline(lm(housing$latitude ~ housing$median_income), col='red')
```

> Findings & Explanation: 

We know that median_income does not have a clear effect on latitude. Similar to the graph of longitude, rich people prefer SF and LA.(The latitude of about -118 is in LA and the latitude of about -122 is in San Francisco.) There is big difference between different areas so it does not make sense to build linear regression model on the whole dataset.

```{r}
m4<- lm(housing_median_age ~ median_income, data = housing)
summary(m4)
plot(housing$median_income, housing$housing_median_age)
abline(lm(housing$housing_median_age ~ housing$median_income), col='red')
```

> Findings & Explanation: 

Although it seems that median_income has a neagtive effect on housing_median_age, the model does not fit very well so we will use other methods to see if there is non-linear relationship in part 2.

```{r}
m5<- lm(total_rooms ~ median_income, data = housing)
summary(m5)
plot(housing$median_income, housing$total_rooms)
abline(lm(housing$total_rooms ~ housing$median_income), col='red')
```

> Findings & Explanation: 

Although it seems that median_income has a positive effect on total_rooms, the model does not fit very well so we will use other methods to see if there is non-linear relationship in part 2.

```{r}
m6<- lm(total_bedrooms ~ median_income, data = housing)
summary(m6)$coefficient
```

> Findings & Explanation: 

Value of Pr(>|t|) = 26.96%>5% which means failure to reject the hypothesis that beta=0, so income is an irrelavant variable to total bedrooms. This means income does not affect choice of communities with different number of bedrooms.

```{r}
m7<- lm(population ~ median_income, data = housing)
summary(m7)$coefficient
```

> Findings & Explanation: 

Value of Pr(>|t|) = 48.74% >5% which means failure to reject the hypothesis that beta=0, so income is an irrelavant variable to population. This means income does not affect choice of communities with population size.

```{r}
m8<- lm(households ~ median_income, data = housing)
summary(m8)
plot(housing$median_income, housing$households)
abline(lm(housing$households ~ housing$median_income), col='red')
```

> Findings & Explanation: 

Value of Pr(>|t|) = 6.12% >5% which means failure to reject the hypothesis that beta=0, so income is an irrelavant variable to households. This means income does not affect choice of communities with different number of households.

```{r}
boxplot(housing$median_income ~ housing$ocean_proximity)
#abline(lm(housing$ocean_proximity ~ housing$median_income))
```

> Findings & Explanation: 

We know that median_income does not have an apparent effect on ocean_proximity. So we will not involve this variable using other methods.

Findings for linear models:

1. Graphs for longitude and latitude show that median_income does not directly affect choice of longitude and latitude because there is big difference between different areas. So we do not think it make sense to build linear regression model on the whole dataset.
2. Total bedrooms, population and households are not statistically significant in linear models.
3. Median_income has positive effect on total rooms and negative effect on housing_median_age. But both linear models do not fit very well.
4. Median_income does not have an apparent effect on ocean_proximity. So we will not involve this variable using other methods.


## 2. Polynomial Regression
```{r}
set.seed(1)
# Set train set and test set
train_ind <- sample(seq_len(nrow(housing)), size = floor(nrow(housing)*0.75))
train_sample <- housing[train_ind,]
test_sample <- housing[-train_ind,]
```


```{r}
# Fit a polynomial model on the training data between housing_median_age and median_income
m4_poly<- lm(housing_median_age ~ poly(median_income, 2),
data = train_sample)
m4_poly1<- lm(housing_median_age ~ poly(median_income, 3),
data = train_sample)
m4_poly2<- lm(housing_median_age ~ poly(median_income, 4),
data = train_sample)
summary(m4_poly)
# Plot the polynomial model
housing %>% 
  ggplot(aes(x = median_income, y= housing_median_age)) + geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2), col = "yellow",se = FALSE)+
  stat_smooth(method = "lm", formula = y ~ poly(x, 3), col = "blue",se = FALSE)+
  stat_smooth(method = "lm", formula = y ~ poly(x, 4), col = "green",se = FALSE)

# Predict housing_median_age using median_income 
predicted1<- predict(m4_poly, newdata = test_sample)
predicted2<- predict(m4_poly1, newdata = test_sample)
predicted3<- predict(m4_poly2, newdata = test_sample)
ggplot(test_sample, aes(x = median_income,y = housing_median_age)) + geom_point()+ geom_line(aes(median_income, predicted1),color = 'Blue')
# model performance
RMSE(predicted1,test_sample$housing_median_age)
R2(predicted1,test_sample$housing_median_age)
RMSE(predicted2,test_sample$housing_median_age)
R2(predicted2,test_sample$housing_median_age)
RMSE(predicted3,test_sample$housing_median_age)
R2(predicted3,test_sample$housing_median_age)
```

> Findings & Explanation: 

Although the line shows slight tendency,all 3 polynomial models do not fit very well so we cannot see direct relationship between median income and housing_median_age. But among them, the model of 2 degree fit best with smallest RMSE and largest R squared. 

```{r}
# Fit a polynomial model on the training data between total_rooms and median_income
m5_poly<- lm(total_rooms ~ poly(median_income, 2),
data = train_sample)
m5_poly1<- lm(total_rooms ~ poly(median_income, 3),
data = train_sample)
m5_poly2<- lm(total_rooms ~ poly(median_income, 4),
data = train_sample)
summary(m5_poly)
# Plot the polynomial model
housing %>% 
  ggplot(aes(x = median_income, y= total_rooms)) + geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2), col = "yellow",se = FALSE)+
    stat_smooth(method = "lm", formula = y ~ poly(x, 3), col = "blue",se = FALSE)+
  stat_smooth(method = "lm", formula = y ~ poly(x, 4), col = "green",se = FALSE)
# Predict total_rooms using median_income 
predicted4<- predict(m5_poly, newdata = test_sample)
predicted5<- predict(m5_poly1, newdata = test_sample)
predicted6<- predict(m5_poly2, newdata = test_sample)
ggplot(test_sample, aes(x = median_income,y = total_rooms)) + geom_point()+ geom_line(aes(median_income, predicted4),color = 'Blue')
# model performance
RMSE(predicted4,test_sample$total_rooms)
R2(predicted4,test_sample$total_rooms)
RMSE(predicted5,test_sample$total_rooms)
R2(predicted5,test_sample$total_rooms)
RMSE(predicted6,test_sample$total_rooms)
R2(predicted6,test_sample$total_rooms)
```

> Findings & Explanation: 

All 3 polynomial models do not fit very well. But among them, the model of 2 degree fits best with smallest RMSE and largest R squared. 

From the graph, we know that when median_income < 7.5, there are a wide range of choice between communities with different number of total rooms. When median_income >7.5, people tend to choose communities with fewer total rooms.

```{r}
# Fit a polynomial model on the training data between total_bedrooms and median_income
m6_poly<- lm(total_bedrooms ~ poly(median_income, 2),
data = train_sample)
m6_poly1<- lm(total_bedrooms ~ poly(median_income, 3),
data = train_sample)
m6_poly2<- lm(total_bedrooms ~ poly(median_income, 4),
data = train_sample)

# Plot the polynomial model
housing %>% 
  ggplot(aes(x = median_income, y= total_bedrooms)) + geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2), col = "yellow",se = FALSE)+
  stat_smooth(method = "lm", formula = y ~ poly(x, 3), col = "blue",se = FALSE)+
  stat_smooth(method = "lm", formula = y ~ poly(x, 4), col = "green",se = FALSE)

# Predict total_bedrooms using median_income 
predicted7<- predict(m6_poly, newdata = test_sample)
predicted8<- predict(m6_poly1, newdata = test_sample)
predicted9<- predict(m6_poly2, newdata = test_sample)
ggplot(test_sample, aes(x = median_income,y = total_bedrooms)) + geom_point()+ geom_line(aes(median_income, predicted8),color = 'Blue')

# model performance
summary(m6_poly)
summary(m6_poly1)
summary(m6_poly2)
```

> Findings & Explanation: 

All 3 polynomial models do not fit very well. But among them, the model of 3 degree fits best with smaller residual standard error and it is not overfitting.Pr(>|t|) value for median_income^1 = 0.475 >0.05 means that median_income^1 does not affect choice of communities of different number of total_bedrooms. But median_income^2 and median_income^3 still affect choice of communities of different number of total_bedrooms.

For those whose income <10, there is no obivous trend between median_income and total_bedrooms. For those whose income > 10, they tend to choose communities with <500 total bedrooms. 

```{r}
# Fit a polynomial model on the training data between population and median_income
m7_poly<- lm(population ~ poly(median_income, 2),
data = train_sample)
m7_poly1<- lm(population ~ poly(median_income, 3),
data = train_sample)
m7_poly2<- lm(population ~ poly(median_income, 4),
data = train_sample)
summary(m7_poly2)
# Plot the polynomial model
housing %>% 
  ggplot(aes(x = median_income, y= population)) + geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2), col = "yellow",se = FALSE)+
   stat_smooth(method = "lm", formula = y ~ poly(x, 3), col = "blue",se = FALSE)+
   stat_smooth(method = "lm", formula = y ~ poly(x, 4), col = "green",se = FALSE)
# Predict population using median_income 
predicted10<- predict(m7_poly, newdata = test_sample)
predicted11<- predict(m7_poly1, newdata = test_sample)
predicted12<- predict(m7_poly2, newdata = test_sample)
ggplot(test_sample, aes(x = median_income,y = population)) + geom_point()+ geom_line(aes(median_income, predicted11),color = 'Blue')
# model performance
RMSE(predicted10,test_sample$population)
R2(predicted10,test_sample$population)
RMSE(predicted11,test_sample$population)
R2(predicted11,test_sample$population)
RMSE(predicted12,test_sample$population)
R2(predicted12,test_sample$population)
```

> Findings & Explanation: 

All 3 polynomial models do not fit very well. But among them, the model of 4 degree fits best with smallest RMSE and biggest R squared.Pr(>|t|) value for median_income^1 = 0.185 >0.05 means that median_income^1 does not affect choice of communities of different population size. But median_income^2, median_income^3 and median_income^4 still affect choice of communities of different population size.

For those whose income <12, there is no obivous trend between median_income and population.For those whose income > 12, people tend to choose communities with population size of <1200.

```{r}
# Fit a polynomial model on the training data between households and median_income
m8_poly<- lm(population ~ poly(median_income, 2),
data = train_sample)
m8_poly1<- lm(population ~ poly(median_income, 3),
data = train_sample)
m8_poly2<- lm(population ~ poly(median_income, 4),
data = train_sample)
summary(m8_poly)
# Plot the polynomial model
housing %>% 
  ggplot(aes(x = median_income, y= households)) + geom_point() +
  stat_smooth(method = "lm", formula = y ~ poly(x, 2), col = "yellow",se = FALSE)+
  stat_smooth(method = "lm", formula = y ~ poly(x, 3), col = "blue",se = FALSE)+
  stat_smooth(method = "lm", formula = y ~ poly(x, 4), col = "green",se = FALSE)

# Predict households using median_income 
predicted13<- predict(m8_poly, newdata = test_sample)
predicted14<- predict(m8_poly1, newdata = test_sample)
predicted15<- predict(m8_poly2, newdata = test_sample)
ggplot(test_sample, aes(x = median_income,y = households)) + geom_point()+ geom_line(aes(median_income, predicted13),color = 'Blue')
# model performance
RMSE(predicted13,test_sample$households)
R2(predicted13,test_sample$households)
RMSE(predicted14,test_sample$households)
R2(predicted14,test_sample$households)
RMSE(predicted15,test_sample$households)
R2(predicted15,test_sample$households)
```

> Findings & Explanation: 

All 3 polynomial models do not fit very well. But among them, the model of 2 degree fits best with smallest RMSE.  Pr(>|t|) value for median_income^1 = 0.185 >0.05 means that median_income^1 does not affect choice of communities of different number of households. But median_income^2 still affect choice of communities of different number of households.

According to the prediction model, for those whose income <10, most people tend to choose communities with fewer than 1500 households but the variety is large. For those whose income>10, people tend to choose households communities with fewer than 500 households. 

Findings for polynomial models:
Generally polynomial models do not fit these 1 to 1 relationships. 1 term of median_income always does not work in polynomial models but higher terms do. Comparatively, the polynomial model for total rooms fits better than for other variables.


## 3. Conclusion for Question 2

##### 1. Linear regression is a basic and commonly used type of predictive analysis which usually works on continuous data. So it is our first trial. But it does not fit well for age and total rooms and it does not make sense for longitude, latitude and ocean proximity. Total bedrooms,population and households are not significant in linear models.

##### 2. Polynomial basically fits a wide range of curvature. So we use it to explore non-linear relationship. But polynomial models still do not fit our dataset.

##### 3. Ridge Regression is a technique used when the data suffers from multicollinearity (independent variables are highly correlated). In our case, it can only be applied to the first question because the second question only has 1 X variable(median_income). 

##### 4. In this question, we can only compare polynomial model with linear regression model for 2 variables(age and total rooms) because some variables are not significant in linear models and some variables are greatly influenced by different areas. For age vs median_income, polynomial model fits better than linear model because of bigger R squared. And result is same with total rooms vs median_income.



